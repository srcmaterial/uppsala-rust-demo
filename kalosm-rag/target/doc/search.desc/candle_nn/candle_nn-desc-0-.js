searchState.loadedDescShard("candle_nn", 0, "candle-nn\nDefining a module with forward method using a single …\nA single forward method using a single single tensor …\nActivation Functions\nBatch Normalization.\nConvolution Layers.\nEmbedding Layer.\nEncoding Utilities. (e.g., one-hot/cold encoding)\nLayers defined by closures.\nGroup Normalization.\nVariable initialization.\nCache Implementations\nLayer Normalization.\nLinear layer\nLoss Calculations\nTensor ops.\nVarious optimization algorithms.\nRecurrent Neural Networks\nRotary Embeddings\nSequential Layer\nA <code>VarBuilder</code> for variable retrieval from models\nA <code>VarMap</code> is a store that holds named variables.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate or initialize a new PReLU layer.\nThe meaning of affine here is different from LayerNorm: …\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nControls exponential moving average of running stats. …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nGet the hidden size of the embedding matrix\nCalls <code>U::from(self)</code>.\nOne-hot/cold encoding.\nA layer defined by a simple closure.\nA layer defined by a simple closure.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nConstant value.\nNumber of features as input or output of a layer. In …\nVariable initializations.\nKaiming uniform initialization. See “Delving deep into …\nThe non-linear function that follows this layer. ReLU is …\nRandom normal with some mean and standard deviation.\nUniform initialization between some lower and upper bounds.\nCompute the fan-in or fan-out value for a weight tensor of …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreates a new tensor with the specified shape, device, and …\nReturns the attn_mask to be applied <em>after</em> adding <code>seq_len</code> …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nRmsNorm is a specialized version of the LayerNorm module.\nFaster variant of the forward kernel, this can only be …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nWhether to remove the mean or not, the default is true and …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCreate or initialize a new linear layer.\nCreate or initialize a new linear layer without biases.\nThe binary cross-entropy with logit loss.\nThe cross-entropy loss.\nThe mean squared error loss.\nThe negative log likelihood loss.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nScaled dot product attention with a fused kernel.\nApplies the softmax function to the input tensor, …\nThe interface optimizers should implement.\nOptimizer for Stochastic Gradient Descent.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nA Gated Recurrent Unit (GRU) layer.\nThe state for a GRU network, this contains a single tensor.\nA Long Short-Term Memory (LSTM) layer.\nThe state for a LSTM network, this contains two tensors.\nTrait for Recurrent Neural Networks.\nThe cell state vector.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nThe hidden state vector, which is also the output of the …\nThe hidden state vector, which is also the output of the …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreates a LSTM layer.\nCreates a LSTM layer.\nCreates a GRU layer.\nApplies multiple steps of the recurrent network.\nApplies multiple steps of the recurrent network.\nConverts a sequence of state to a tensor.\nApplies a single step of the recurrent network.\nA zero state from which the recurrent network is usually …\nA sequential layer combining multiple other layers.\nAppends a layer after all the current layers.\nAppends a closure after all the current layers.\nApplies the forward pass and returns the output for each …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nReturns true if this layer does not have any sub-layer.\nThe number of sub-layers embedded in this layer.\nCreates a new empty sequential layer.\nA trait that defines how tensor data is retrieved.\nThis traits specifies a way to rename the queried names …\nA simple <code>VarBuilder</code>, this is less generic than …\nA structure used to retrieve variables, these variables …\nThis returns true only if a tensor with the passed in name …\nThe device used by default.\nThe dtype used by default.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nInitializes a <code>VarBuilder</code> using a custom backend.\nInitializes a <code>VarBuilder</code> using a custom backend.\nInitializes a <code>VarBuilder</code> from a binary buffer in the …\nInitializes a <code>VarBuilder</code> from a binary buffer in the …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> from a binary slice in the …\nInitializes a <code>VarBuilder</code> from a binary slice in the …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> using a <code>VarMap</code>. The requested …\nInitializes a <code>VarBuilder</code> using a <code>VarMap</code>. The requested …\nRetrieve a tensor with some target shape.\nRetrieve a tensor based on a target name and shape.\nRetrieve the tensor associated with the given name at the …\nRetrieve the tensor associated with the given name at the …\nRetrieve the tensor associated with the given name &amp; dtype …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nShort alias for <code>push_prefix</code>.\nReturns the prefix of the <code>VarBuilder</code>.\nReturn a new <code>VarBuilder</code> adding <code>s</code> to the current prefix. …\nThis is applied to the name obtained by a name call and …\nGets a VarBuilder that applies some renaming function on …\nGets a VarBuilder that applies some renaming function on …\nReturns a new <code>VarBuilder</code> using the root path.\nReturns a new <code>VarBuilder</code> with the prefix set to <code>prefix</code>.\nClone the VarBuilder tweaking its dtype\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …\nInitializes a <code>VarBuilder</code> that uses zeros for any tensor.\nInitializes a <code>VarBuilder</code> that uses zeros for any tensor.\nA <code>VarMap</code> is a store that holds named variables. Variables …\nRetrieve all the variables currently stored in the map.\nReturns the argument unchanged.\nRetrieve or add a new variable.\nCalls <code>U::from(self)</code>.\nLoad some values from a safetensors file and modify the …\nCreate a new empty <code>VarMap</code>.\nSave the map in the safetensors format.\nSet some named variables to some values.\nSet a named variable to some value.")