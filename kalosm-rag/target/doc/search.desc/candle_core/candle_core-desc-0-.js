searchState.loadedDescShard("candle_core", 0, "ML framework for Rust\nUnary ops that can be defined in user-land.\nThe different types of elements allowed in tensors.\nCpu, Cuda, or Metal\nA <code>DeviceLocation</code> represents a physical device whereas …\nTrait used to implement multiple signatures for ease of …\nIndexing via a 1d tensor\nUnary ops that can be defined in user-land. These ops work …\nDefining a module with forward method using a single …\nA single forward method using a single single tensor …\nThis is a regular slice, purely indexing a chunk of the …\nThis selects the elements for which an index has some …\nAn iterator over offset position for items of an …\nThe core struct for manipulating tensors.\nUnique identifier for tensors.\nGeneric structure used to index a slice of the tensor\nA variable is a wrapper around a tensor, however variables …\nThis operation multiplies the input tensor by <code>mul</code> then …\nRun the <code>forward</code> method of <code>m</code> on <code>self</code>.\nApplies a unary custom op.\nApplies a unary custom op without backward support\nApplies a binary custom op.\nApplies a binary custom op without backward support\nApplies a ternary custom op.\nApplies a ternary custom op without backward support\nRun the <code>forward</code> method of <code>m</code> on <code>self</code>.\nCreates a new 1D tensor with values from the interval …\nCreates a new 1D tensor with values from the interval …\nReturns the indices that sort the tensor along the last …\nSimilar to <code>argmax_keepdim</code> but the target dimension is …\nSimilar to <code>argmin_keepdim</code> but the target dimension is …\nString representation for dtypes.\n2D average pooling over an input tensor with multiple …\nSame as <code>avg_pool2d</code> but with a <code>stride</code> that can be set to a …\nTraits to Define Backend Behavior\nMethods for backpropagation of gradients.\nReturn <code>BF16</code> for devices that support it, otherwise default …\nBroadcast the input tensor to the target shape. This …\nReturns a new tensor duplicating data from the original …\nMatrix-multiplication with broadcasting support.\nBroadcasting version of <code>pow</code>.\nThis function takes as argument the argument <code>arg</code> used in …\nThis function takes as argument the argument <code>arg</code> used in …\nConcatenates two or more tensors along a particular …\nSplit a tensor into the specified number of chunks, this …\nClamp the tensor values to be between <code>min</code> and <code>max</code>.\nElement-wise comparison between two tensors, e.g. …\nReturns a tensor that is in row major order. This is the …\n1D and 2D Convolutions\nApplies a 1D convolution over the input tensor.\nApplies a 2D convolution over the input tensor.\nApplies a 1D transposed convolution over the input tensor.\nApplies a 2D transposed convolution over the input tensor.\nCompared to clone, this copies the actual storage but may …\nTraits and methods for CPU-backed Tensors\nImplementation of Backend Fns for CPU\nThe forward pass, as run on a cpu device. Note that the …\nThe forward pass, as run on a cpu device. Note that the …\nThe forward pass, as run on a cpu device. Note that the …\nThe forward pass, as run on a cpu device. Note that the …\nThe forward pass, as run on a cpu device. Note that the …\nThe forward pass, as run on a cpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nThe forward pass, as run on a gpu device. Note that the …\nReturns the cumulative sum of elements of the input tensor …\nReturns a new tensor detached from the current graph, …\nThe device on which the input tensor is located.\nThe dimension size for a specified dimension index.\nThe dimension size for this tensor on each axis.\nPretty printing of tensors\nThe dtype for the elements stored in the input tensor.\nImplementation of the Cuda backend when Cuda support has …\nThe number of elements stored in this tensor.\nApplies the Exponential Linear Unit (ELU) function on each …\nReturns a tensor with the values from the <code>self</code> tensor at …\nElement-wise equality.\nCandle-specific Error and Result\nAn alias for broadcast_as.\nReturns a matrix with a diagonal of ones of size n by n.\nFlattens the input tensor on the dimension indexes from …\nFlattens the input tensor by reshaping it into a one …\nFlattens the input tensor on the dimension indexes from …\nFlattens the input tensor on the dimension indexes from <code>0</code> …\nReturns a tensor that is in row major order. This always …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCreates a new 1D tensor from an iterator.\nCreates a new tensor initialized with values from the …\nCreates a new tensor initialized with values from the …\nReturns a new tensor with all the elements having the same …\nGather values across the target dimension.\nElement-wise comparison with greater-equal, the returned …\nReturns the sub-tensor fixing the index at <code>i</code> on the first …\nReturns the sub-tensor fixing the index at <code>index</code> on the …\nElement-wise comparison with greater-than, the returned …\nReturns a slicing iterator which are the chunks of data …\nsee [TensorIndex#method.i]\nsee [TensorIndex#method.i]\nsee [TensorIndex#method.i]\nsee [TensorIndex#method.i]\nsee [TensorIndex#method.i]\nThe unique identifier for this tensor.\nAccumulate element from <code>source</code> at indexes <code>indexes</code> and add …\nSelect values for the input tensor at the target indexes …\nApplies a unary custom op in place.\nApplies a unary custom op in place (for the first tensor).\nApplies a ternary custom op in place (for the first …\nInterpolate the input tensor to the <code>target_size</code> size, …\nInterpolate the input tensor to the <code>(target_h, target_w)</code> …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nConsumes this <code>Var</code> and return the underlying tensor.\nReturns true if the data is stored in a C contiguous (aka …\nReturns true if the data is stored in a Fortran contiguous …\nWhether this tensor is a variable or not. A variable is a …\nTensor Layouts including contiguous or sparse strides\nThe layout of the input tensor, this stores both the shape …\nElement-wise comparison with lower-equal, the returned …\nReturns log(sum(exp(tensor), dim)).\nElement-wise comparison with lower-than, the returned …\nReturns the matrix-multiplication of the input tensor with …\nSimilar to <code>max_keepdim</code> but the target dimension is …\nComputes the max of all the elements in this tensor and …\nGathers the maximum value across the selected dimension. …\n2D max pooling over an input tensor with multiple channels.\nSame as <code>max_pool2d</code> but with a <code>stride</code> that can be set to a …\nReturns the mean of all elements in the input tensor. The …\nReturns the mean of all elements in the input tensor. The …\nCreates grids of coordinates specified by the 1D inputs.\nImplementation of Backend traits for Metal\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nThe forward pass, as run on a metal gpu device. Note that …\nSimilar to <code>min_keepdim</code> but the target dimension is …\nComputes the min of all the elements in this tensor and …\nGathers the minimum value across the selected dimension. …\nReturns a new tensor that is a narrowed version of the …\nElement-wise non-equality.\nCreates a new tensor on the specified device using the …\nCreates a new tensor on the specified device using the …\nNormalize a ‘relative’ axis value: positive values are …\nNumpy support for tensors.\nCreates a new tensor filled with ones.\nCreates a new tensor filled with ones with same shape, …\nTensor Opertion Enums and Traits\nPad the input tensor using same values along dimension <code>dim</code>…\nPad the input tensor using 0s along dimension <code>dim</code>. This …\nReturns a tensor with the same data as the input where the …\nJust enough pickle support to be able to read PyTorch …\nPointwise pow operation.\nRaise the tensor to some float exponent <code>e</code>.\nCode for GGML and GGUF files\nCreates a new tensor initialized with values sampled …\nCreates a new tensor initialized with values sampled from …\nThe number of dimensions for this tensor, 0 for a scalar …\nReads a npy file and return the stored multi-dimensional …\nReads a npz file and returns the stored multi-dimensional …\nReads a npz file and returns the stored multi-dimensional …\nRepeat this tensor along the specified dimensions.\nReshape returns a tensor with the target shape provided …\nRoll the tensor input along the given dimension. Elements …\nRound element of the input tensor to the nearest integer.\nModule to load <code>safetensor</code> files into CPU/GPU memory.\nTensorScalar Enum and Trait\nSets the content of the inner tensor, this does not …\nThe shape of a tensor is a tuple with the size of each of …\nThe tensor shape, i.e. dimension sizes on each axis.\nThe size used by each element in bytes, i.e. 1 for <code>U8</code>, 4 …\nReturns a copy of <code>self</code> where the values within <code>ranges</code> have …\nEmbeds the values of the <code>src</code> tensor into the <code>self</code> tensor …\nEmbeds the values of the <code>src</code> tensor into the <code>self</code> tensor …\nSet the values on <code>self</code> using values from <code>src</code>. The copy …\nSorts the tensor along the last dimension, returns the …\nReturn all the nodes that lead to this value in a …\nCreates a new tensor with the specified dimension removed …\nStacks two or more tensors along a particular dimension.\nThe storage used by this tensor, together with the layout …\nStreamTensror useful for streaming ops.\nSimilar to <code>strided_index</code> but returns the position of the …\nReturns an iterator over position of the elements in the …\nReturns the sum of all elements in the input tensor. The …\nComputes the sum of all the elements in this tensor and …\nReturns the sum of all elements in the input tensor. The …\nReturns a tensor that is a transposed version of the …\nIf the target device is the same as the tensor device, …\nCasts the input tensor to the target <code>dtype</code>.\nRetrieves the single scalar value hold in the tensor. If …\nAn alias for <code>to_scalar</code>.\nReturns the data contained in a 1D tensor as a vector of …\nReturns the data contained in a 2D tensor as a vector of …\nReturns the data contained in a 3D tensor.\nReturns true if the computation graph should track this …\nReturns a tensor that is a transposed version of the …\nReturns a lower triangular matrix of ones of size n by n.\nReturns an upper triangular matrix of ones of size n by n.\nCreates a new tensor with a dimension of size one inserted …\nAlias for <code>interpolate1d</code>.\nAlias for <code>interpolate2d</code>.\nUseful functions for checking features.\nReturns the unbiased variance over the selected dimension.\nReturns the unbiased variance over the selected dimension.\nReturns a tensor with the same shape as the input tensor, …\nWrites a multi-dimensional array in the npy format.\nWrites multiple multi-dimensional arrays using the npz …\nCreates a new tensor filled with zeros.\nCreates a new tensor filled with zeros with same shape, …\nSafety\nSynchronize should block until all the operations on the …\nA store for gradients, associating a tensor id to the …\nReturns the argument unchanged.\nGet the gradient tensor associated with the given tensor\nGet the gradient tensor corresponding to the given tensor …\nGet the tensor ids of the stored gradient tensors\nInsert a gradient tensor associated with the given tensor, …\nCalls <code>U::from(self)</code>.\nRemove the gradient tensor associated with the given …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nProvides the error and related functions\n<code>erf</code> calculates the error function at <code>x</code>.\n<code>erf_inv</code> calculates the inverse error function at <code>x</code>.\n<code>erfc</code> calculates the complementary error function at <code>x</code>.\n<code>erfc_inv</code> calculates the complementary inverse error …\nDot-product of two vectors.\nMaximum element in a non-empty vector.\nMinimum element in a non-empty vector.\nSum of all elements in a vector.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nOptions for Tensor pretty printing\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nReturns the argument unchanged.\nThis bool controls whether reduced precision reductions …\nThis bool controls whether reduced precision reductions …\nThis bool controls whether reduced precision reductions …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThis bool controls whether reduced precision reductions …\nThis bool controls whether reduced precision reductions …\nThis bool controls whether reduced precision reductions …\nContains the error value\nMain library error type.\nUtf8 parse error.\nI/O error.\nUser generated error message, typically created via <code>bail!</code>.\nContains the success value\nInteger parse error.\nSafeTensor error.\nAdding path information to an error.\nArbitrary errors wrapping.\nZip file format error.\nWrap the error value with additional context.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nWrap the error value with additional context that is …\nReturns the appropriate start and stop offset if the data …\nThe dimension size for a specified dimension index.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns true if the data is stored in a C contiguous (aka …\nReturns true if the data is stored in a Fortran contiguous …\nUnique identifier for cuda devices.\nSimple way to catch lock error without depending on T\nMetal related errors\nCreate a metal GPU capture trace on [<code>path</code>].\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreates a new buffer (not necessarily zeroed). The buffer …\nCreates a new buffer (not necessarily zeroed). The buffer …\nCreates a new buffer from data. The buffer is MTLManaged\nLazy tensor loader.\nReturns the argument unchanged.\nThis only returns the shape and dtype for a named tensor. …\nCalls <code>U::from(self)</code>.\n<code>BackpropOp</code> is a wrapper around <code>Option&lt;Op&gt;</code>. The main goal …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLazy tensor loader.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nRead all the tensors from a PyTorch pth file.\nRead all the tensors from a PyTorch pth file with a given …\nRead the tensor info from a .pth file.\nThe block size, i.e. the number of elements stored in each …\nThe block dtype\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nSupport for the GGML file format.\nSupport for the GGUF file format.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThe type size for blocks in bytes.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreates a Tensor from a raw GGML tensor.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThis will also automatically upcast any integral types …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nDot product used as a building block for quantized mat-mul.\nGeneric implementation of the dot product without simd …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreates a wrapper around multiple memory mapped file and …\nCreates a wrapper around a memory mapped file and …\nCreates a wrapper around a binary buffer and deserialize …\nCreates a wrapper around a binary buffer and deserialize …\nCreates a wrapper around a memory mapped file from which …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCheck whether the two shapes are compatible for broadcast, …\nThe dimension size for a specified dimension index.\nThe dimensions as a slice of <code>usize</code>.\nThe total number of elements, this is the product of all …\nModifies the shape by adding a list of additional …\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns true if the strides are C contiguous (aka row …\nReturns true if the strides are Fortran contiguous (aka …\nThe rank is the number of dimensions, 0 for a scalar …\nSimple wrapper that doesn’t do any buffering.\nA stream tensor is used in streaming module. It can either …\nStreaming modules take as input a stream tensor and return …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nSplits the Streaming Tensor on the time axis <code>dim</code> with the …")