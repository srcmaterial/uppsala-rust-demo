searchState.loadedDescShard("tokenizers", 0, "The core of <code>tokenizers</code>, written in Rust. Provides an …\nImplement <code>serde::{Serialize, Serializer}</code> with …\nPopular tokenizer models.\nRepresents a tokenization pipeline.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nAllows decoding Original BPE by joining all the tokens and …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nByteFallback is a simple trick which converts tokens …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nThe CTC (Connectionist Temporal Classification) decoder …\nWhether to cleanup some tokenization artifacts. Mainly …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nThe pad token used by CTC to delimit a new token.\nThe word delimiter token. It will be replaced by a <code>&lt;space&gt;</code>.\nFuse simply fuses all tokens into one big string. It’s …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nStrip is a simple trick which converts tokens looking like …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nThe WordPiece decoder takes care of decoding a list of …\nWhether to cleanup some tokenization artifacts (spaces …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nThe prefix to be used for continuing subwords\nByte Pair Encoding model.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nUnigram model.\nWordPiece model.\nA Byte Pair Encoding model.\nWhen the merges.txt file is in the wrong format. This …\nWhen the vocab.json file is in the wrong format\nA <code>BpeBuilder</code> can be used to create a <code>BPE</code> model with a …\nErrors that can be encountered while using or constructing …\nDropout not between 0 and 1.\nAn error encountered while reading files mainly.\nAn error forwarded from Serde, while parsing JSON\nIf a token found in merges, is not in the vocab\nIf the provided unk token is out of vocabulary\nReturns a <code>BPE</code> model that uses the <code>BpeBuilder</code>’s …\nInitialize a <code>BpeBuilder</code>.\nSet the <code>byte_fallback</code> option.\nByte fallback from sentence pieces, instead of UNK, uses …\nSet the cache’s capacity. Set to 0 if you want to …\nReset the cache.\nSet the <code>continuing_subword_prefix</code> option.\nAn optional prefix to use on any subword that exist only …\nUse dropout with the model.\nDropout probability for merges. 0.0 = no dropout is the …\nSet the <code>end_of_word_suffix</code> option.\nAn optional suffix to caracterize and end-of-word subword\nSet the input files.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nInitialize a BpeBuilder model from vocab and merges files\nSet the <code>fuse_unk</code> option.\nDo multiple unk tokens get fused\nSet the <code>ignore_merges</code> option.\nWhether or not to direct output words if they are part of …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nConstructs a new <code>BpeBuilder</code>.\nCreate a new BPE model with the given vocab and merges.\nRead the given files to extract the vocab and merges\nResize the cache\nSet the <code>UNK</code> token for the vocab.\nThe unknown token to be used when we encounter an unknown …\nSet the vocab (token -&gt; ID) and merges mappings.\nIn charge of training a <code>BPE</code> model\nA <code>BpeTrainerBuilder</code> can be used to create a <code>BpeTrainer</code> …\nConstructs the final BpeTrainer\nSet the continuing_subword_prefix\nAn optional prefix to use on any subword that exist only …\nSet the end_of_word_suffix\nAn optional suffix to caracterize and end-of-word subword\nReturns the argument unchanged.\nReturns the argument unchanged.\nSet the initial alphabet\nThe initial alphabet we want absolutely to include. This …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nSet whether to limit the alphabet\nWhether to limit the number of initial tokens that can be …\nSet max_token_length\nAn optional parameter to limit the max length of any …\nSet the expected minimum frequency\nThe minimum frequency a pair must have to produce a merge …\nConstructs a new <code>BpeTrainerBuilder</code>\nWhether we should show progress\nSet whether to show progress\nWhether to show progress while training\nSet the special tokens\nA list of special tokens that the model should know of\nTrain a BPE model\nSet the vocabulary size\nThe target vocabulary size\nStructure to implement Viterbi algorithm to find the best …\nA node from the lattice, that helps reconstruct the …\nA <code>Unigram</code> model to encode sentences.\nIterator to iterate of vocabulary of the model, and their …\nA <code>UnigramTrainer</code> can train a <code>Unigram</code> model from <code>word_counts</code>…\nBuilder for <code>UnigramTrainer</code>.\nError type for UnigramTrainerBuilder\nUninitialized field\nCustom validation error\nBuilds a new <code>UnigramTrainer</code>.\nClears the internal cache\nThis functions take a String, and will encode it in a Vec …\nReturns the argument unchanged.\nCreate a <code>Unigram</code> model from a given vocabulary. Vocabulary …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nIterate of vocabulary of the model as a pair of …\nLoads a SentencePiece output model after being trained by …\nResize the cache\nWhether we should show progress\nTrain a Unigram model\nUninitialized field\nCustom validation error\nA <code>WordLevelBuilder</code> can be used to create a <code>WordLevel</code> model …\nBuilder for <code>WordLevelTrainer</code>.\nError type for WordLevelTrainerBuilder\nContructs a <code>WordLevel</code> model that uses the <code>WordLevelBuilder</code>…\nBuilds a new <code>WordLevelTrainer</code>.\nSet the input files.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nInitialize a WordLevel model from vocab and merges file.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThe minimum frequency a word must have to be part of the …\nThe minimum frequency a word must have to be part of the …\nConstruct a new <code>WordLevelBuilder</code>.\nWhether we should show progress\nWhether to show progress while training\nWhether to show progress while training\nA list of special tokens that the model should know of\nA list of special tokens that the model should know of\nTrain a WordLevel model\nThe the <code>UNK</code> token for the vocab.\nSet the vocab (token -&gt; ID) mapping.\nThe target vocabulary size\nThe target vocabulary size\nA WordPiece model.\nA <code>WordPieceBuilder</code> can be used to create a <code>WordPiece</code> model …\nTrains a <code>WordPiece</code> model.\nA <code>WordPieceTrainerBuilder</code> can be used to create a …\nConstructs the final BpeTrainer\nContructs a <code>WordPiece</code> model that uses the <code>WordPieceBuilder</code>…\nGet a <code>WordPieceBuilder</code>.\nSet the continuing_subword_prefix\nSet the prefix for continuing subwords.\nSet the end_of_word_suffix\nSet the input files.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCreate a <code>WordPiece</code> model from a <code>BPE</code> model.\nInitialize a <code>WordPiece</code> model from a vocab mapping file.\nSet the initial alphabet\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nSet whether to limit the alphabet\nSet the maximum number of input characters per word.\nSet the expected minimum frequency\nConstructs a new <code>WordPieceTrainerBuilder</code>\nConstruct a new <code>WordPieceBuilder</code>.\nRead the given files to extract the vocab\nSet whether to show progress\nSet the special tokens\nThe the <code>UNK</code> token for the vocab.\nSet the vocab (token -&gt; ID) mapping.\nSet the vocabulary size\nWrapper for known Normalizers.\nThis struct is specifically done to be compatible with …\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nWhether to do the bert basic cleaning:\nReturns the argument unchanged.\nWhether to put spaces around chinese characters so they …\nCalls <code>U::from(self)</code>.\nWhether to lowercase the input\nWhether to strip accents\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nStrip the normalized string inplace\nThis struct is specifically done to be compatible with …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nStrip the normalized string inplace\nThis normalizer will take a <code>pattern</code> (for now only a String)\nRepresents the different patterns that <code>Replace</code> can use\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nStrip the normalized string inplace\nStrip the normalized string inplace\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLowercases the input\nAllows concatenating multiple other Normalizer as a …\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nProvides all the necessary steps to handle the BPE …\nWhether to add a leading space to the first word. This …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nWhether the post processing step should trim offsets to …\nWhether to use the standard GPT2 regex for whitespace …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nPre tokenizes the numbers into single tokens. If …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nSpecifies that the scheme should always be prepended.\nSpecifies that the scheme should be prepended only once, …\nReplaces all the whitespaces by the provided meta …\nSpecifies that the space should not be prepended.\nEnum representing options for the metaspace prepending …\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nRepresents the different patterns that <code>Split</code> can use\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nTemplate Processing\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nThis is the first sequence, the one that is always …\nThis is the pair sequence, that is optional\nRepresents the different kind of pieces that constitute a …\nRepresents any sequences received as input of the …\nRepresents a bunch of tokens to be used in a template. …\nA Template represents a Vec&lt;<code>Piece</code>&gt;.\nThis PostProcessor takes care of processing each input …\nBuilder for <code>TemplateProcessing</code>.\nError type for TemplateProcessingBuilder\nA bunch of <code>SpecialToken</code> represented by their ID. …\nUninitialized field\nCustom validation error\nBuilds a new <code>TemplateProcessing</code>.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nRepresent a token added by the user on top of the existing …\nA vocabulary built on top of the Model\nDecodeStream will keep the state necessary to produce …\nA <code>Decoder</code> changes the raw tokens into its more readable …\nRepresents the output of a <code>Tokenizer</code>.\nContains the error value\nRepresents a model used during Tokenization (like BPE or …\nTakes care of pre-processing strings.\nContains the success value\nA <code>PostProcessor</code> has the responsibility to post process an …\nThe <code>PreTokenizer</code> is in charge of doing the …\nBuilder for Tokenizer structs.\nA <code>Tokenizer</code> is capable of encoding/decoding any text.\nA <code>Trainer</code> has the responsibility to train a model. We feed …\nAdd some special tokens to the vocabulary\nRegister the given tokens as special tokens. This is …\nAdd some tokens to the vocabulary\nAdd the given tokens to the added vocabulary\nReturns the number of tokens that will be added during the …\nConvert the TokenizerBuilder to a Tokenizer.\nGet the token that contains the given char.\nGet the word that contains the given char.\nThe content of the added token\nDecode the given ids, back to a String\nDecode all sentences in parallel\nDecode the given ids, back to a String See <code>DecodeStream</code>\nEncode the given input. This method accepts both single …\nEncode all the sentences in parallel, using multiple …\nEncode all the sentences in parallel, using multiple …\nEncode all the sentences in parallel, using multiple …\nEncode the given input, using offsets relative to chars …\nEncode the given input. This method accepts both single …\nExtract the additional vocabulary from the given sentence, …\nProcess an iterator of sequences, calling <code>process</code> for each …\nReturns the argument unchanged.\nReturns the argument unchanged.\nBuild this token from the given content, specifying if it …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nInstantiate a new Tokenizer from bytes\nInstantiate a new Tokenizer from the given file\nGet the additional vocabulary with the AddedTokens\nGet the added tokens decoder\nGet the added vocabulary\nGet the decoder\nGet added token value\nGet the model\nGet the normalizer\nGet the currently set padding parameters\nGet a mutable reference to the currently set padding …\nGet the post processor\nGet the pre tokenizer\nGet an instance of a Trainer capable of training this Model\nGet the currently set truncation parameters\nGet a mutable reference to the currently set truncation …\nRetrieve the entire vocabulary mapping (token -&gt; ID)\nGet the additional vocabulary\nGet the vocabulary\nRetrieve the size of the vocabulary\nGet the size of the vocabulary\nFind the string token associated to an ID\nGet the token matching the given id if it exists\nConverts an id to the corresponding token.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nUnwrap the TokenizerImpl.\nWhether or not this vocabulary is empty\nWhether this Encoding is empty\nCheck if a token is a special token\nSize of the additional vocabulary\nReturn the total length of this Encoding\nSpecify whether this token should include all the …\nWhether this token should strip whitespaces on its left\nMerge all Encodings together\nMerge ourself with the given <code>Encoding</code>. Happens in place.\nReturn the number of sequences combined in this Encoding\nGet an empty TokenizerBuilder.\nConstruct a new Tokenizer based on the model.\nInstantiate a new Tokenizer, with the given Model\nSpecify whether this token should be normalized and match …\nWhether this token should be normalized\nPost processing logic, handling the case where there is no …\nProcess both encodings and returns a new merged one\nProcess any amount of encodings and returns a series of …\nSpecify whether this token should include all the …\nWhether this token should strip whitespaces on its right\nSave the current <code>Model</code> in the given folder, using the …\nSave the current tokenizer at the given path\nset the added bocab’s splitting scheme\nSet the given sequence id for the whole range of tokens …\nWhether we should show progress during the training.\nSpecify whether this token should only match on whole …\nWhether this token must be a single word or can break words\nSpecify whether this token is special, meaning if it …\nWhether this token is special\nSee <code>DecodeStream</code>\nInternal function exposed only to bypass python limitations\nSerialize the current tokenizer as a String\nGet the offsets of the token at the given index.\nFind the ID associated to a string token\nGet the id matching one of our token if it exists\nConverts a token in the corresponding id.\nReturns the index of the sequence containing the given …\nGet the word that contains the token at the given index.\nTokenize the given sequence into multiple underlying <code>Token</code>…\nThe actual training method. This will return a new trained …\nTrain our Model, using the given Trainer and iterator\nTrain our Model from files\nTruncate the current <code>Encoding</code>.\nSet the added vocabulary.\nSet the added vocabulary.\nSet the decoder.\nSet the decoder\nSet the model.\nSet the model\nSet the normalizer.\nSet the normalizer\nSet the padding parameters.\nSet the padding parameters\nSet the post-processor.\nSet the post processor\nSet the pre-tokenizer.\nSet the pre tokenizer\nSet the trunaction parameters.\nSet the truncation parameters\nGet the offsets of the word at the given index in the …\nGet the encoded tokens corresponding to the word at the …\nA <code>NormalizedString</code> takes care of processing an “original…\nThe possible offsets referential\nRepresents a Range usable by the NormalizedString to index …\nDefines the expected behavior for the delimiter of a Split …\nAppend the given string to ourself\nConvert the given range from bytes to char\nConvert the given range from char to bytes\nClear the normalized part of the string\nConvert the given offsets range from one referential to …\nApplies filtering over our characters\nCalls the given function for each characters\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturn the normalized string\nReturn the original string\nReturn a range of the normalized string\nReturns a range of the given string slice, by indexing …\nReturn a range of the original string\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nConverts the current Range to a <code>std::ops::Range&lt;usize&gt;</code>. …\nWhether empty\nReturn the length of the current Range if not Unbounded\nReturns the length of the normalized string (counting …\nReturns the length of the original string (counting chars …\nLowercase\nRemove any leading space(s) of the normalized string\nMap our characters\nApplies NFC normalization\nApplies NFD normalization\nApplies NFKC normalization\nApplies NFKD normalization\nReturn the original offsets\nPrepend the given string to ourself\nReplace anything that matches the pattern with the given …\nRemove any trailing space(s) of the normalized string\nReturn a slice of the current NormalizedString If the …\nSplit the current string in many subparts. Specify what to …\nRemove any leading and trailing space(s) of the normalized …\nApplies transformations to the current normalized version …\nApplies transformations to the current normalized version …\nUnwrap the underlying range\nUppercase\nInvert the <code>is_match</code> flags for the wrapped Pattern. This is …\nPattern used to split a NormalizedString\nSlice the given string in a list of pattern match …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nVarious possible types of offsets\nThe <code>PreTokenizedString</code> is in charge of splitting an …\nWrapper for a subpart of a <code>NormalizedString</code>.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns a list of splits, each of them being a slice of …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nTransform the current <code>PreTokenizedString</code> into an <code>Encoding</code>.\nNormalized all the splits that do not have attached <code>Tokens</code>…\nSplit the <code>PreTokenizedString</code> by providing a <code>split_fn</code> in …\nTokenize all the splits that do not have attached <code>Tokens</code>, …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nThis comes from the Rust libcore and is duplicated here …\nThis module defines helpers to allow optional Rayon usage.\nCopied from std::io::BufRead but keep newline characters.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nProcess the given iterator as if it yielded a <code>T</code> instead of …\nThe various possible padding directions.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nConverts any serial iterator into a CondIterator, that can …\nAllows to convert into an iterator that can be executed …\nShared reference version of MaybeParallelIterator, works …\nExclusive reference version of MaybeParallelIterator, …\nAllows to convert into <code>chunks</code> that can be executed either …\nReturns the number of threads in the current registry. If …\nCheck if at some point we used a parallel iterator\nConvert ourself in a CondIterator, that will be executed …\nConvert ourself in a CondIterator, that will be executed …\nCheck if the TOKENIZERS_PARALLELISM env variable has been …\nCreate a CondIterator, that will be executed either in …\nCreate a CondIterator, that will be executed either in …\nSet the value for <code>TOKENIZERS_PARALLELISM</code> for the current …\nWe are supposed to truncate the pair sequence, but it has …\nWe cannot truncate the target sequence enough to respect …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.")