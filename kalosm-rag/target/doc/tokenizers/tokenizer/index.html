<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Represents a tokenization pipeline."><title>tokenizers::tokenizer - Rust</title><script>if(window.location.protocol!=="file:")document.head.insertAdjacentHTML("beforeend","SourceSerif4-Regular-6b053e98.ttf.woff2,FiraSans-Italic-81dc35de.woff2,FiraSans-Regular-0fe48ade.woff2,FiraSans-MediumItalic-ccf7e434.woff2,FiraSans-Medium-e1aa3f0a.woff2,SourceCodePro-Regular-8badfe75.ttf.woff2,SourceCodePro-Semibold-aa29a496.ttf.woff2".split(",").map(f=>`<link rel="preload" as="font" type="font/woff2" crossorigin href="../../static.files/${f}">`).join(""))</script><link rel="stylesheet" href="../../static.files/normalize-9960930a.css"><link rel="stylesheet" href="../../static.files/rustdoc-916cea96.css"><meta name="rustdoc-vars" data-root-path="../../" data-static-root-path="../../static.files/" data-current-crate="tokenizers" data-themes="" data-resource-suffix="" data-rustdoc-version="1.87.0 (17067e9ac 2025-05-09)" data-channel="1.87.0" data-search-js="search-e7298875.js" data-settings-js="settings-d72f25bb.js" ><script src="../../static.files/storage-82c7156e.js"></script><script defer src="../sidebar-items.js"></script><script defer src="../../static.files/main-fb8c74a8.js"></script><noscript><link rel="stylesheet" href="../../static.files/noscript-893ab5e7.css"></noscript><link rel="icon" href="https://huggingface.co/favicon.ico"></head><body class="rustdoc mod"><!--[if lte IE 11]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><nav class="mobile-topbar"><button class="sidebar-menu-toggle" title="show sidebar"></button><a class="logo-container" href="../../tokenizers/index.html"><img src="https://huggingface.co/landing/assets/huggingface_logo.svg" alt=""></a></nav><nav class="sidebar"><div class="sidebar-crate"><a class="logo-container" href="../../tokenizers/index.html"><img src="https://huggingface.co/landing/assets/huggingface_logo.svg" alt="logo"></a><h2><a href="../../tokenizers/index.html">tokenizers</a><span class="version">0.21.1</span></h2></div><div class="sidebar-elems"><section id="rustdoc-toc"><h2 class="location"><a href="#">Module tokenizer</a></h2><h3><a href="#reexports">Module Items</a></h3><ul class="block"><li><a href="#reexports" title="Re-exports">Re-exports</a></li><li><a href="#modules" title="Modules">Modules</a></li><li><a href="#structs" title="Structs">Structs</a></li><li><a href="#enums" title="Enums">Enums</a></li><li><a href="#traits" title="Traits">Traits</a></li><li><a href="#functions" title="Functions">Functions</a></li><li><a href="#types" title="Type Aliases">Type Aliases</a></li></ul></section><div id="rustdoc-modnav"><h2 class="in-crate"><a href="../index.html">In crate tokenizers</a></h2></div></div></nav><div class="sidebar-resizer"></div><main><div class="width-limiter"><rustdoc-search></rustdoc-search><section id="main-content" class="content"><div class="main-heading"><div class="rustdoc-breadcrumbs"><a href="../index.html">tokenizers</a></div><h1>Module <span>tokenizer</span><button id="copy-path" title="Copy item path to clipboard">Copy item path</button></h1><rustdoc-toolbar></rustdoc-toolbar><span class="sub-heading"><a class="src" href="../../src/tokenizers/tokenizer/mod.rs.html#1-1556">Source</a> </span></div><details class="toggle top-doc" open><summary class="hideme"><span>Expand description</span></summary><div class="docblock"><p>Represents a tokenization pipeline.</p>
<p>A <a href="struct.Tokenizer.html"><code>Tokenizer</code></a> is composed of some of the following parts.</p>
<ul>
<li><a href="trait.Normalizer.html"><code>Normalizer</code></a>: Takes care of the text normalization (like unicode normalization).</li>
<li><a href="trait.PreTokenizer.html"><code>PreTokenizer</code></a>: Takes care of the pre tokenization (ie. How to split tokens and pre-process
them.</li>
<li><a href="trait.Model.html"><code>Model</code></a>: A model encapsulates the tokenization algorithm (like BPE, Word base, character
based, …).</li>
<li><a href="trait.PostProcessor.html"><code>PostProcessor</code></a>: Takes care of the processing after tokenization (like truncating, padding,
…).</li>
</ul>
</div></details><h2 id="reexports" class="section-header">Re-exports<a href="#reexports" class="anchor">§</a></h2><dl class="item-table reexports"><dt id="reexport.DecoderWrapper"><code>pub use crate::decoders::<a class="enum" href="../decoders/enum.DecoderWrapper.html" title="enum tokenizers::decoders::DecoderWrapper">DecoderWrapper</a>;</code></dt><dt id="reexport.ModelWrapper"><code>pub use crate::models::<a class="enum" href="../models/enum.ModelWrapper.html" title="enum tokenizers::models::ModelWrapper">ModelWrapper</a>;</code></dt><dt id="reexport.NormalizerWrapper"><code>pub use crate::normalizers::<a class="enum" href="../normalizers/enum.NormalizerWrapper.html" title="enum tokenizers::normalizers::NormalizerWrapper">NormalizerWrapper</a>;</code></dt><dt id="reexport.PreTokenizerWrapper"><code>pub use crate::pre_tokenizers::<a class="enum" href="../pre_tokenizers/enum.PreTokenizerWrapper.html" title="enum tokenizers::pre_tokenizers::PreTokenizerWrapper">PreTokenizerWrapper</a>;</code></dt><dt id="reexport.PostProcessorWrapper"><code>pub use crate::processors::<a class="enum" href="../processors/enum.PostProcessorWrapper.html" title="enum tokenizers::processors::PostProcessorWrapper">PostProcessorWrapper</a>;</code></dt><dt id="reexport.LinesWithEnding"><code>pub use crate::utils::iter::<a class="trait" href="../utils/iter/trait.LinesWithEnding.html" title="trait tokenizers::utils::iter::LinesWithEnding">LinesWithEnding</a>;</code></dt><dt id="reexport.pad_encodings"><code>pub use crate::utils::padding::<a class="fn" href="../utils/padding/fn.pad_encodings.html" title="fn tokenizers::utils::padding::pad_encodings">pad_encodings</a>;</code></dt><dt id="reexport.PaddingDirection"><code>pub use crate::utils::padding::<a class="enum" href="../utils/padding/enum.PaddingDirection.html" title="enum tokenizers::utils::padding::PaddingDirection">PaddingDirection</a>;</code></dt><dt id="reexport.PaddingParams"><code>pub use crate::utils::padding::<a class="struct" href="../utils/padding/struct.PaddingParams.html" title="struct tokenizers::utils::padding::PaddingParams">PaddingParams</a>;</code></dt><dt id="reexport.PaddingStrategy"><code>pub use crate::utils::padding::<a class="enum" href="../utils/padding/enum.PaddingStrategy.html" title="enum tokenizers::utils::padding::PaddingStrategy">PaddingStrategy</a>;</code></dt><dt id="reexport.truncate_encodings"><code>pub use crate::utils::truncation::<a class="fn" href="../utils/truncation/fn.truncate_encodings.html" title="fn tokenizers::utils::truncation::truncate_encodings">truncate_encodings</a>;</code></dt><dt id="reexport.TruncationDirection"><code>pub use crate::utils::truncation::<a class="enum" href="../utils/truncation/enum.TruncationDirection.html" title="enum tokenizers::utils::truncation::TruncationDirection">TruncationDirection</a>;</code></dt><dt id="reexport.TruncationParams"><code>pub use crate::utils::truncation::<a class="struct" href="../utils/truncation/struct.TruncationParams.html" title="struct tokenizers::utils::truncation::TruncationParams">TruncationParams</a>;</code></dt><dt id="reexport.TruncationStrategy"><code>pub use crate::utils::truncation::<a class="enum" href="../utils/truncation/enum.TruncationStrategy.html" title="enum tokenizers::utils::truncation::TruncationStrategy">TruncationStrategy</a>;</code></dt><dt id="reexport.NormalizedString"><code>pub use normalizer::<a class="struct" href="normalizer/struct.NormalizedString.html" title="struct tokenizers::tokenizer::normalizer::NormalizedString">NormalizedString</a>;</code></dt><dt id="reexport.OffsetReferential"><code>pub use normalizer::<a class="enum" href="normalizer/enum.OffsetReferential.html" title="enum tokenizers::tokenizer::normalizer::OffsetReferential">OffsetReferential</a>;</code></dt><dt id="reexport.SplitDelimiterBehavior"><code>pub use normalizer::<a class="enum" href="normalizer/enum.SplitDelimiterBehavior.html" title="enum tokenizers::tokenizer::normalizer::SplitDelimiterBehavior">SplitDelimiterBehavior</a>;</code></dt><dt><code>pub use <a class="mod" href="pre_tokenizer/index.html" title="mod tokenizers::tokenizer::pre_tokenizer">pre_tokenizer</a>::*;</code></dt></dl><h2 id="modules" class="section-header">Modules<a href="#modules" class="anchor">§</a></h2><dl class="item-table"><dt><a class="mod" href="normalizer/index.html" title="mod tokenizers::tokenizer::normalizer">normalizer</a></dt><dt><a class="mod" href="pattern/index.html" title="mod tokenizers::tokenizer::pattern">pattern</a></dt><dt><a class="mod" href="pre_tokenizer/index.html" title="mod tokenizers::tokenizer::pre_tokenizer">pre_<wbr>tokenizer</a></dt></dl><h2 id="structs" class="section-header">Structs<a href="#structs" class="anchor">§</a></h2><dl class="item-table"><dt><a class="struct" href="struct.AddedToken.html" title="struct tokenizers::tokenizer::AddedToken">Added<wbr>Token</a></dt><dd>Represent a token added by the user on top of the existing Model vocabulary.
AddedToken can be configured to specify the behavior they should have in various situations
like:</dd><dt><a class="struct" href="struct.AddedVocabulary.html" title="struct tokenizers::tokenizer::AddedVocabulary">Added<wbr>Vocabulary</a></dt><dd>A vocabulary built on top of the Model</dd><dt><a class="struct" href="struct.BuilderError.html" title="struct tokenizers::tokenizer::BuilderError">Builder<wbr>Error</a></dt><dt><a class="struct" href="struct.DecodeStream.html" title="struct tokenizers::tokenizer::DecodeStream">Decode<wbr>Stream</a></dt><dd>DecodeStream will keep the state necessary to produce individual chunks of
strings given an input stream of token_ids.</dd><dt><a class="struct" href="struct.Encoding.html" title="struct tokenizers::tokenizer::Encoding">Encoding</a></dt><dd>Represents the output of a <code>Tokenizer</code>.</dd><dt><a class="struct" href="struct.Token.html" title="struct tokenizers::tokenizer::Token">Token</a></dt><dt><a class="struct" href="struct.Tokenizer.html" title="struct tokenizers::tokenizer::Tokenizer">Tokenizer</a></dt><dt><a class="struct" href="struct.TokenizerBuilder.html" title="struct tokenizers::tokenizer::TokenizerBuilder">Tokenizer<wbr>Builder</a></dt><dd>Builder for Tokenizer structs.</dd><dt><a class="struct" href="struct.TokenizerImpl.html" title="struct tokenizers::tokenizer::TokenizerImpl">Tokenizer<wbr>Impl</a></dt><dd>A <code>Tokenizer</code> is capable of encoding/decoding any text.</dd><dt><a class="struct" href="struct.TruncationParamError.html" title="struct tokenizers::tokenizer::TruncationParamError">Truncation<wbr>Param<wbr>Error</a></dt></dl><h2 id="enums" class="section-header">Enums<a href="#enums" class="anchor">§</a></h2><dl class="item-table"><dt><a class="enum" href="enum.DecodeStreamError.html" title="enum tokenizers::tokenizer::DecodeStreamError">Decode<wbr>Stream<wbr>Error</a></dt><dt><a class="enum" href="enum.EncodeInput.html" title="enum tokenizers::tokenizer::EncodeInput">Encode<wbr>Input</a></dt><dt><a class="enum" href="enum.InputSequence.html" title="enum tokenizers::tokenizer::InputSequence">Input<wbr>Sequence</a></dt><dt><a class="enum" href="enum.ProcessorError.html" title="enum tokenizers::tokenizer::ProcessorError">Processor<wbr>Error</a></dt></dl><h2 id="traits" class="section-header">Traits<a href="#traits" class="anchor">§</a></h2><dl class="item-table"><dt><a class="trait" href="trait.Decoder.html" title="trait tokenizers::tokenizer::Decoder">Decoder</a></dt><dd>A <code>Decoder</code> changes the raw tokens into its more readable form.</dd><dt><a class="trait" href="trait.Model.html" title="trait tokenizers::tokenizer::Model">Model</a></dt><dd>Represents a model used during Tokenization (like BPE or Word or Unigram).</dd><dt><a class="trait" href="trait.Normalizer.html" title="trait tokenizers::tokenizer::Normalizer">Normalizer</a></dt><dd>Takes care of pre-processing strings.</dd><dt><a class="trait" href="trait.PostProcessor.html" title="trait tokenizers::tokenizer::PostProcessor">Post<wbr>Processor</a></dt><dd>A <code>PostProcessor</code> has the responsibility to post process an encoded output of the <code>Tokenizer</code>.
It adds any special tokens that a language model would require.</dd><dt><a class="trait" href="trait.PreTokenizer.html" title="trait tokenizers::tokenizer::PreTokenizer">PreTokenizer</a></dt><dd>The <code>PreTokenizer</code> is in charge of doing the pre-segmentation step. It splits the given string
in multiple substrings, keeping track of the offsets of said substrings from the
<code>NormalizedString</code>. In some occasions, the <code>PreTokenizer</code> might need to modify the given
<code>NormalizedString</code> to ensure we can entirely keep track of the offsets and the mapping with
the original string.</dd><dt><a class="trait" href="trait.Trainer.html" title="trait tokenizers::tokenizer::Trainer">Trainer</a></dt><dd>A <code>Trainer</code> has the responsibility to train a model. We feed it with lines/sentences
and then it can train the given <code>Model</code>.</dd></dl><h2 id="functions" class="section-header">Functions<a href="#functions" class="anchor">§</a></h2><dl class="item-table"><dt><a class="fn" href="fn.step_decode_stream.html" title="fn tokenizers::tokenizer::step_decode_stream">step_<wbr>decode_<wbr>stream</a></dt><dd>Internal function exposed only to bypass python limitations</dd></dl><h2 id="types" class="section-header">Type Aliases<a href="#types" class="anchor">§</a></h2><dl class="item-table"><dt><a class="type" href="type.Error.html" title="type tokenizers::tokenizer::Error">Error</a></dt><dt><a class="type" href="type.Offsets.html" title="type tokenizers::tokenizer::Offsets">Offsets</a></dt><dt><a class="type" href="type.Result.html" title="type tokenizers::tokenizer::Result">Result</a></dt></dl></section></div></main></body></html>